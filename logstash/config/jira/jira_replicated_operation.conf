input {
    file {
        path => "/usr/share/logstash/logs/**/atlassian-jira.log*"
        mode => "read"
        file_completed_action => "log"
        file_completed_log_path => "/tmp/replicated_index.log"
        sincedb_path => "/tmp/replicated_index"
    }
}

filter {
  grok {
    match => {
      "message" => [
           "(?m)%{TIMESTAMP_ISO8601:timestamp} %{NOTSPACE:thread_name} %{NOTSPACE:loglevel}      \[%{NOTSPACE:classname}\] Node replay index operations stats: %{WORD}\=%{NOTSPACE:replication_node} numberOfOperations=%{NUMBER:numberOfOperations}\, timeToReplay=%{NUMBER:timeToReplay}ms, errors=%{NUMBER:errors}, period=%{NUMBER:period} %{WORD:interval}"
            ]
    }       
  }
      # we're not interested in lines that fail or don't match so we will just drop them. 
        if "_grokparsefailure" in [tags] {
          drop { }
        }

        date {
          match => [ "timestamp" , "ISO8601"]
          target => "@timestamp"
        }

      # let's convert our number fields to integer so we can graph them in kibana :).
        mutate {
        convert => { "numberOfOperations" => "integer" }
        convert => { "timeToReplay" => "integer" }
        convert => { "errors" => "integer" }
        convert => { "period" => "integer" }
        }

        grok {
            match => { "path" => "%{UNIXPATH:subpath}" }
        }
        mutate {
            split => { "subpath" => "/" }
        }

        ruby {
            code => "event.set('node',event.get('subpath')[6]); event.set('supportzip', event.get('subpath').length > 2 ? event.get('subpath')[]:'none')"
        }

}



output {
    elasticsearch {
        hosts => "elasticsearch:9200"
        index => "replicated_index-%{+YYYY.MM.dd}"
    }
}
